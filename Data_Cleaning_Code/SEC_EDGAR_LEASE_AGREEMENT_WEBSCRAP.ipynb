{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting urlib2\n",
      "\u001b[31m  Could not find a version that satisfies the requirement urlib2 (from versions: )\u001b[0m\n",
      "\u001b[31mNo matching distribution found for urlib2\u001b[0m\n",
      "\u001b[33mYou are using pip version 18.0, however version 18.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "#!pip install urlib2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import urllib.request\n",
    "import time\n",
    "from sys import stdout\n",
    "from time import sleep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP 1: GET NEXT-PAGE-CURSOR AND ALL LEASE URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Characteristics of this website: 1/ Infinite scroll down with no next page button 2/Log in Required to download files\n",
    "get_url function is to web scrap each next-page url and corresponding lease doc url\n",
    "paramters: page - starting page to web scrap\n",
    "           cursor_prefix - the prefix of each page url\n",
    "           page_limite - limit of pages to scroll down\n",
    "           \n",
    "key variable: url- a dictionary with lease doc url as keys, and its unique suffix as values\n",
    "output: page_url - a dictionary with page url as key, and corresponding url dictionary as values\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "def get_url(page,curor_prefix, page_limit=100):\n",
    "    \n",
    "    page_url=dict()\n",
    "    count=0\n",
    "    #page = 'https://www.lawinsider.com/contracts/tagged/lease-agreement'\n",
    "    #cursor_prefix='https://www.lawinsider.com/contracts/tagged/lease-agreement?cursor='\n",
    "\n",
    "\n",
    "    while (len(page)>0) and (len(page_url.keys())<=page_limit):\n",
    "       \n",
    "            count+=1\n",
    "            url=dict()\n",
    "            r = requests.get(page)\n",
    "            soup = BeautifulSoup(r.text, \"lxml\")\n",
    "            for sub in soup.find(id='pagination-append').find_all(href=re.compile(r'/contracts/(.*)')):\n",
    "                url['https://www.lawinsider.com{}'.format(sub['href'])]=re.compile(r'(/contracts/([\\d\\w]+))/(.*)').search(sub['href']).group(3).replace('/','_')\n",
    "\n",
    "            page_url[page]=url\n",
    "            stdout.write('\\r{} pages to scroll down'.format(str(page_limit+1-count)))\n",
    "            stdout.flush()\n",
    "            sleep(1)\n",
    "            page=cursor_prefix+soup.find(id='pagination-append').find('a').attrs['data-next-cursor']\n",
    "        #except:\n",
    "            #print('ERROR')\n",
    "            #continue\n",
    "        \n",
    "    return page_url\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 pages to scroll down"
     ]
    }
   ],
   "source": [
    "##TEST CASE\n",
    "\n",
    "url_dict=get_url('https://www.lawinsider.com/contracts/tagged/lease-agreement',\n",
    "       'https://www.lawinsider.com/contracts/tagged/lease-agreement?cursor=',\n",
    "       page_limit=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP 2: WEB SCRAP URLs Using SELENIUM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#!pip install selenium\n",
    "\"\"\"\n",
    "Need to download the webdriver which is webbrowser-specific \n",
    "For chrome: http://chromedriver.chromium.org/downloads\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SELENIUM LOGIN AND SET THE FILE DOWNLOAD DIRECTORY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "\n",
    "dl_dir = \"/Users/yanbincen/Desktop/missionpossible2019/sample_data_download\"\n",
    "chrome_path = \"/Users/yanbincen/anaconda/lib/python3.6/site-packages/chromedriver\"\n",
    "chromeOptions = webdriver.ChromeOptions()\n",
    "prefs = {\"download.default_directory\" : dl_dir}\n",
    "chromeOptions.add_experimental_option(\"prefs\",prefs)\n",
    "driver = webdriver.Chrome(executable_path=chrome_path, chrome_options=chromeOptions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CLICK LOG IN PAGE AND FILL OUT LIG ON INFO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "log_in_page = 'https://accounts.google.com/signin/oauth/identifier?client_id=948740859141-jj4vlg2tqi6fqj62q7mvbo8jjam6dclv.apps.googleusercontent.com&as=Onvu0NDyehK7gm0A8Rd1kA&destination=https%3A%2F%2Fwww.lawinsider.com&approval_state=!ChR6SE1fSjhUTG5pbjQwd2ZYUVZ4QhIfNDZ2cnNraGQyZWNYQU9FaERQUmpBazJxT19UYmhCWQâˆ™APNbktkAAAAAXD4ufnraS8L9ERp8GB3Vj80bukBBQMeA&oauthgdpr=1&oauthriskyscope=1&xsrfsig=ChkAeAh8T_pYE_tn-OJaCsz09ZIo9DWoUW-tEg5hcHByb3ZhbF9zdGF0ZRILZGVzdGluYXRpb24SBXNvYWN1Eg9vYXV0aHJpc2t5c2NvcGU&flowName=GeneralOAuthFlow'\n",
    "email = 'missionpossible2019nyc@gmail.com'\n",
    "password = 'missionimpossible1999'\n",
    "driver.get(log_in_page)\n",
    "time.sleep(5)\n",
    "\n",
    "driver.find_element_by_id(\"identifierId\").send_keys(email)\n",
    "driver.find_element_by_id(\"identifierNext\").click()\n",
    "time.sleep(5)\n",
    "driver.find_element_by_name(\"password\").send_keys(password)\n",
    "element = driver.find_element_by_id('passwordNext')\n",
    "driver.execute_script(\"arguments[0].click();\", element)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MULTIPLE DOWNLOAD TEST CASE; DAILY LIMIT 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "105"
      ]
     },
     "execution_count": 416,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_url=[k for ud in url_dict.values() for k in ud.keys()]\n",
    "p=re.compile(r'(https://www.lawinsider.com/contracts/([\\S]+))/([\\S-]+)/\\d+/([\\S]+)')\n",
    "\n",
    "for url in test_url:\n",
    "   \n",
    "    company_name = p.search(url).group(3)\n",
    "    date = p.search(url).group(4)\n",
    "    doc_url=p.search(url).group(1)+'.docx'\n",
    "    driver.get(doc_url)\n",
    "    print('{} left; {} done'.format(len(test_url)-1-test_url.index(url),url))\n",
    "    #stdout.write('\\r{} done'.format(u))\n",
    "    #stdout.flush()\n",
    "    #time.sleep(5)\n",
    "    #if os.path.isfile(dl_dir+'/{}.docx'):\n",
    "        #os.rename(dl_dir+'/{}.docx'.format(p.search(url).group(2)),\n",
    "             #dl_dir+'/{}_{}.docx'.format(company_name,date))\n",
    "    #else:\n",
    "        #print(dl_dir+'/{}.docx'.format(p.search(url).group(2))+'stuck')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
